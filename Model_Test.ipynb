{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b02664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Models import *\n",
    "from Layers import *\n",
    "\n",
    "import Activations as a\n",
    "import Loss_Functions as lf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "np.set_printoptions(precision = 4,suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b091abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printdif(paramname,torchval, myval):\n",
    "    print(\"#\"*50)\n",
    "    print(f\"{paramname}:\\nMine:\\n{myval}\\n\\nPytorch's:\\n{torchval.detach().numpy()}\")\n",
    "    print()\n",
    "    print(f\"Sum Difference:\\n{np.sum(np.abs(myval - torchval.detach().numpy()))}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self, imwidth, imheight, \n",
    "                 inchannels, outchannels, \n",
    "                 kernelsize, outsize, \n",
    "                 stride = (1,1), padding = (0,0)):\n",
    "        super().__init__()\n",
    "\n",
    "        out_h = int((imheight + 2 * padding[0]- kernelsize[0]) / stride[0] + 1)\n",
    "        out_w = int((imwidth + 2 * padding[1] - kernelsize[1]) / stride[1] + 1)\n",
    "        \n",
    "        self.conv = nn.Conv2d(inchannels, outchannels, kernelsize, stride = stride, padding = padding)\n",
    "        self.fc = nn.Linear(outchannels * out_h * out_w, outsize)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.reshape(out.size(0),-1)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888786fb",
   "metadata": {},
   "source": [
    "## Comparing outputs and gradients of a model with a conv2d layer and linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda44f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "outputs:\n",
      "Mine:\n",
      "[[-0.1782  0.0566  0.0256]\n",
      " [-0.0877  0.3313  0.017 ]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.1782  0.0566  0.0256]\n",
      " [-0.0877  0.3313  0.017 ]]\n",
      "\n",
      "Sum Difference:\n",
      "1.510139682886491e-07\n",
      "\n",
      "##################################################\n",
      "loss:\n",
      "Mine:\n",
      "1.0602596228002306\n",
      "\n",
      "Pytorch's:\n",
      "1.0602595806121826\n",
      "\n",
      "Sum Difference:\n",
      "4.218804794398068e-08\n",
      "\n",
      "##################################################\n",
      "output gradient:\n",
      "Mine:\n",
      "[[-0.3568  0.1811  0.1756]\n",
      " [ 0.1377 -0.2906  0.1529]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.3568  0.1811  0.1756]\n",
      " [ 0.1377 -0.2906  0.1529]]\n",
      "\n",
      "Sum Difference:\n",
      "9.22599028285731e-08\n",
      "\n",
      "##################################################\n",
      "fc weight gradient:\n",
      "Mine:\n",
      "[[-0.0696  0.      0.      0.0423  0.     -0.2924 -0.2428 -0.0089  0.0411\n",
      "   0.     -0.1306 -0.193 ]\n",
      " [-0.2022  0.      0.     -0.0893  0.      0.0895  0.1085  0.0045 -0.2084\n",
      "   0.      0.0663  0.0765]\n",
      " [ 0.2718  0.      0.      0.047   0.      0.2029  0.1343  0.0044  0.1673\n",
      "   0.      0.0643  0.1165]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.0696  0.      0.      0.0423  0.     -0.2924 -0.2428 -0.0089  0.0411\n",
      "   0.     -0.1306 -0.193 ]\n",
      " [-0.2022  0.      0.     -0.0893  0.      0.0895  0.1085  0.0045 -0.2084\n",
      "   0.      0.0663  0.0765]\n",
      " [ 0.2718  0.      0.      0.047   0.      0.2029  0.1343  0.0044  0.1673\n",
      "   0.      0.0643  0.1165]]\n",
      "\n",
      "Sum Difference:\n",
      "4.772624829655139e-07\n",
      "\n",
      "##################################################\n",
      "fc bias gradient:\n",
      "Mine:\n",
      "[-0.2191 -0.1095  0.3285]\n",
      "\n",
      "Pytorch's:\n",
      "[-0.2191 -0.1095  0.3285]\n",
      "\n",
      "Sum Difference:\n",
      "5.575037045901787e-08\n",
      "\n",
      "##################################################\n",
      "conv weight gradient:\n",
      "Mine:\n",
      "[[[[-0.1199  0.0348 -0.0687  0.113 ]\n",
      "   [ 0.1359  0.0367  0.0618 -0.043 ]\n",
      "   [-0.0685 -0.0885 -0.0261  0.1308]\n",
      "   [-0.0651  0.0675 -0.03   -0.0141]]\n",
      "\n",
      "  [[ 0.0391  0.0527  0.0146 -0.0479]\n",
      "   [-0.0891  0.0464 -0.0334 -0.1221]\n",
      "   [ 0.0554  0.1125  0.1989  0.1351]\n",
      "   [ 0.0665 -0.0553 -0.0371  0.0219]]\n",
      "\n",
      "  [[-0.2313 -0.0066 -0.1749 -0.1272]\n",
      "   [ 0.0802 -0.1017  0.1655  0.0369]\n",
      "   [ 0.0488 -0.0184 -0.1566  0.0318]\n",
      "   [-0.0305  0.1281  0.0231 -0.0713]]]\n",
      "\n",
      "\n",
      " [[[-0.0247 -0.1104  0.2699 -0.1585]\n",
      "   [ 0.1791 -0.1424  0.1053  0.1855]\n",
      "   [-0.404   0.0061  0.092   0.1202]\n",
      "   [-0.0829 -0.0008  0.0564 -0.0799]]\n",
      "\n",
      "  [[ 0.056   0.0341 -0.0951  0.0343]\n",
      "   [-0.0659 -0.0297 -0.0676 -0.1129]\n",
      "   [-0.1607  0.2396  0.1107  0.0547]\n",
      "   [ 0.002  -0.0993  0.007  -0.024 ]]\n",
      "\n",
      "  [[-0.1176  0.0739  0.0392 -0.0951]\n",
      "   [ 0.0959  0.0012  0.1991 -0.1193]\n",
      "   [-0.0568  0.1629 -0.3027  0.1236]\n",
      "   [-0.0234  0.1474  0.0008 -0.1192]]]]\n",
      "\n",
      "Pytorch's:\n",
      "[[[[-0.1199  0.0348 -0.0687  0.113 ]\n",
      "   [ 0.1359  0.0367  0.0618 -0.043 ]\n",
      "   [-0.0685 -0.0885 -0.0261  0.1308]\n",
      "   [-0.0651  0.0675 -0.03   -0.0141]]\n",
      "\n",
      "  [[ 0.0391  0.0527  0.0146 -0.0479]\n",
      "   [-0.0891  0.0464 -0.0334 -0.1221]\n",
      "   [ 0.0554  0.1125  0.1989  0.1351]\n",
      "   [ 0.0665 -0.0553 -0.0371  0.0219]]\n",
      "\n",
      "  [[-0.2313 -0.0066 -0.1749 -0.1272]\n",
      "   [ 0.0802 -0.1017  0.1655  0.0369]\n",
      "   [ 0.0488 -0.0184 -0.1566  0.0318]\n",
      "   [-0.0305  0.1281  0.0231 -0.0713]]]\n",
      "\n",
      "\n",
      " [[[-0.0247 -0.1104  0.2699 -0.1585]\n",
      "   [ 0.1791 -0.1424  0.1053  0.1855]\n",
      "   [-0.404   0.0061  0.092   0.1202]\n",
      "   [-0.0829 -0.0008  0.0564 -0.0799]]\n",
      "\n",
      "  [[ 0.056   0.0341 -0.0951  0.0343]\n",
      "   [-0.0659 -0.0297 -0.0676 -0.1129]\n",
      "   [-0.1607  0.2396  0.1107  0.0547]\n",
      "   [ 0.002  -0.0993  0.007  -0.024 ]]\n",
      "\n",
      "  [[-0.1176  0.0739  0.0392 -0.0951]\n",
      "   [ 0.0959  0.0012  0.1991 -0.1193]\n",
      "   [-0.0568  0.1629 -0.3027  0.1236]\n",
      "   [-0.0234  0.1474  0.0008 -0.1192]]]]\n",
      "\n",
      "Sum Difference:\n",
      "9.586801752448082e-07\n",
      "\n",
      "##################################################\n",
      "conv bias gradient:\n",
      "Mine:\n",
      "[ 0.0893 -0.072 ]\n",
      "\n",
      "Pytorch's:\n",
      "[ 0.0893 -0.072 ]\n",
      "\n",
      "Sum Difference:\n",
      "2.3376164018373835e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set hyperparams\n",
    "imwidth = 6\n",
    "imheight = 5\n",
    "inchannels = 3\n",
    "outchannels = 2\n",
    "kernelsize = (4,4)\n",
    "outsize = 3\n",
    "batchsize = 2\n",
    "padding = (0,0)\n",
    "stride = (1,1)\n",
    "out_h = int((imheight + 2 * padding[0]- kernelsize[0]) / stride[0] + 1)\n",
    "out_w = int((imwidth + 2 * padding[1] - kernelsize[1]) / stride[1] + 1)\n",
    "\n",
    "#Instantiate Torch Model\n",
    "simpleconv = SimpleConv(imwidth, imheight,\n",
    "                        inchannels, outchannels,\n",
    "                        kernelsize, outsize)\n",
    "  \n",
    "#Create Numpy Model\n",
    "model = Model()\n",
    "model.addLayer(Conv2d(inchannels,outchannels, kernelsize), activation = a.ReLU())\n",
    "model.addLayer(Flatten())\n",
    "model.addLayer(Linear(batchsize * out_h * out_w, outsize))\n",
    "model.set_loss_fn(lf.CrossEntropyLoss())\n",
    "\n",
    "#Load Torch weights into Numpy Model\n",
    "model.torch_weighted(simpleconv)\n",
    "\n",
    "#Input and ytrue\n",
    "torchin = torch.randn((batchsize,inchannels,imheight,imwidth))\n",
    "torchtrue = torch.randint(outsize,(batchsize,))\n",
    "\n",
    "npin = torchin.detach().numpy()\n",
    "nptrue = torchtrue.detach().numpy()\n",
    "\n",
    "#Get model outputs\n",
    "torchout = simpleconv(torchin)\n",
    "torchout.retain_grad()\n",
    "npout = model(npin)\n",
    "printdif(\"outputs\", torchout, npout)\n",
    "\n",
    "#Calculate losses\n",
    "torchloss = nn.CrossEntropyLoss()(torchout, torchtrue)\n",
    "nploss = model.calculate_loss(nptrue)\n",
    "printdif(\"loss\", torchloss, nploss)\n",
    "\n",
    "#Calculate gradients\n",
    "torchloss.backward(retain_graph = True)\n",
    "model.backward()\n",
    "printdif(\"output gradient\", torchout.grad, model.loss_fn.gradient)\n",
    "printdif(\"fc weight gradient\", simpleconv.fc.weight.grad, model.layers[2].gradients[0])\n",
    "printdif(\"fc bias gradient\", simpleconv.fc.bias.grad, model.layers[2].gradients[1])\n",
    "printdif(\"conv weight gradient\", simpleconv.conv.weight.grad, model.layers[0].gradients[0])\n",
    "printdif(\"conv bias gradient\", simpleconv.conv.bias.grad, model.layers[0].gradients[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a7696",
   "metadata": {},
   "source": [
    "## Comparing outputs and gradients of a 2 layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27eeb2d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Outputs:\n",
      "Mine:\n",
      "[[-0.21   -0.3226 -0.1606 -0.1621  0.485 ]\n",
      " [-0.4966  0.1495 -0.0338  0.0855  0.2625]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.21   -0.3226 -0.1606 -0.1621  0.485 ]\n",
      " [-0.4966  0.1495 -0.0338  0.0855  0.2625]]\n",
      "\n",
      "Sum Difference:\n",
      "1.2293457984924316e-07\n",
      "\n",
      "##################################################\n",
      "Loss:\n",
      "Mine:\n",
      "1.637831211090088\n",
      "\n",
      "Pytorch's:\n",
      "1.637831211090088\n",
      "\n",
      "Sum Difference:\n",
      "0.0\n",
      "\n",
      "##################################################\n",
      "Output gradient:\n",
      "Mine:\n",
      "[[ 0.0834 -0.4255  0.0876  0.0875  0.1671]\n",
      " [ 0.0594  0.1133  0.0943  0.1062 -0.3732]]\n",
      "\n",
      "Pytorch's:\n",
      "[[ 0.0834 -0.4255  0.0876  0.0875  0.1671]\n",
      " [ 0.0594  0.1133  0.0943  0.1062 -0.3732]]\n",
      "\n",
      "Sum Difference:\n",
      "5.21540641784668e-08\n",
      "\n",
      "##################################################\n",
      "layer 1 weight_ih gradient:\n",
      "Mine:\n",
      "[[-0.0015  0.0006  0.0007  0.0002  0.0004]\n",
      " [ 0.0016 -0.0023 -0.0006 -0.0006  0.0001]\n",
      " [-0.0021  0.0005  0.0011  0.0001  0.0007]\n",
      " [-0.0011  0.0007  0.0005  0.0002  0.0002]\n",
      " [ 0.0025  0.0027 -0.0018  0.0007 -0.0019]\n",
      " [-0.0015  0.0012  0.0006  0.0003  0.0002]\n",
      " [ 0.0142 -0.0029 -0.0073 -0.0008 -0.0047]\n",
      " [-0.0016  0.0004  0.0008  0.0001  0.0005]\n",
      " [-0.0015 -0.0002  0.0009 -0.      0.0007]\n",
      " [ 0.0046  0.0026 -0.0029  0.0007 -0.0027]\n",
      " [ 0.0074 -0.0031 -0.0036 -0.0008 -0.0019]\n",
      " [-0.0517  0.0073  0.0272  0.0021  0.0183]\n",
      " [ 0.0054 -0.006  -0.002  -0.0016 -0.0001]\n",
      " [ 0.0055 -0.0035 -0.0025 -0.0009 -0.001 ]\n",
      " [ 0.0049  0.011  -0.0043  0.0028 -0.0057]\n",
      " [-0.0042  0.002   0.002   0.0005  0.001 ]\n",
      " [ 0.0234 -0.0056 -0.012  -0.0015 -0.0075]\n",
      " [-0.0037  0.001   0.0019  0.0003  0.0012]\n",
      " [-0.0048 -0.0004  0.0027 -0.0001  0.0021]\n",
      " [ 0.0197  0.0065 -0.0118  0.0016 -0.0102]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.0015  0.0006  0.0007  0.0002  0.0004]\n",
      " [ 0.0016 -0.0023 -0.0006 -0.0006  0.0001]\n",
      " [-0.0021  0.0005  0.0011  0.0001  0.0007]\n",
      " [-0.0011  0.0007  0.0005  0.0002  0.0002]\n",
      " [ 0.0025  0.0027 -0.0018  0.0007 -0.0019]\n",
      " [-0.0015  0.0012  0.0006  0.0003  0.0002]\n",
      " [ 0.0142 -0.0029 -0.0073 -0.0008 -0.0047]\n",
      " [-0.0016  0.0004  0.0008  0.0001  0.0005]\n",
      " [-0.0015 -0.0002  0.0009 -0.      0.0007]\n",
      " [ 0.0046  0.0026 -0.0029  0.0007 -0.0027]\n",
      " [ 0.0074 -0.0031 -0.0036 -0.0008 -0.0019]\n",
      " [-0.0517  0.0073  0.0272  0.0021  0.0183]\n",
      " [ 0.0054 -0.006  -0.002  -0.0016 -0.0001]\n",
      " [ 0.0055 -0.0035 -0.0025 -0.0009 -0.001 ]\n",
      " [ 0.0049  0.011  -0.0043  0.0028 -0.0057]\n",
      " [-0.0042  0.002   0.002   0.0005  0.001 ]\n",
      " [ 0.0234 -0.0056 -0.012  -0.0015 -0.0075]\n",
      " [-0.0037  0.001   0.0019  0.0003  0.0012]\n",
      " [-0.0048 -0.0004  0.0027 -0.0001  0.0021]\n",
      " [ 0.0197  0.0065 -0.0118  0.0016 -0.0102]]\n",
      "\n",
      "Sum Difference:\n",
      "8.646384230814874e-08\n",
      "\n",
      "##################################################\n",
      "layer 1 weight_hh gradient:\n",
      "Mine:\n",
      "[[ 0.0015  0.0007  0.0006  0.0004 -0.002 ]\n",
      " [-0.006   0.0002 -0.001   0.0002  0.0047]\n",
      " [ 0.0013  0.0013  0.0007  0.0006 -0.0023]\n",
      " [ 0.0018  0.0004  0.0005  0.0002 -0.0019]\n",
      " [ 0.007  -0.0036 -0.0001 -0.002  -0.0023]\n",
      " [ 0.003   0.0004  0.0007  0.0001 -0.0028]\n",
      " [-0.0074 -0.0088 -0.0047 -0.0044  0.0145]\n",
      " [ 0.001   0.0009  0.0005  0.0005 -0.0017]\n",
      " [-0.0004  0.0012  0.0004  0.0006 -0.0009]\n",
      " [ 0.0069 -0.0051 -0.0007 -0.0027 -0.0007]\n",
      " [-0.0081 -0.0035 -0.0028 -0.0017  0.0101]\n",
      " [ 0.0188  0.0339  0.0163  0.0171 -0.0482]\n",
      " [-0.0156 -0.0002 -0.0029  0.0002  0.013 ]\n",
      " [-0.0091 -0.0019 -0.0024 -0.0008  0.0093]\n",
      " [ 0.0287 -0.0106  0.0012 -0.006  -0.0132]\n",
      " [ 0.0052  0.0018  0.0017  0.0009 -0.0061]\n",
      " [-0.0144 -0.0139 -0.0079 -0.0069  0.0253]\n",
      " [ 0.0025  0.0022  0.0013  0.0011 -0.0042]\n",
      " [-0.0012  0.0039  0.0013  0.002  -0.0028]\n",
      " [ 0.017  -0.0188 -0.004  -0.01    0.0043]]\n",
      "\n",
      "Pytorch's:\n",
      "[[ 0.0015  0.0007  0.0006  0.0004 -0.002 ]\n",
      " [-0.006   0.0002 -0.001   0.0002  0.0047]\n",
      " [ 0.0013  0.0013  0.0007  0.0006 -0.0023]\n",
      " [ 0.0018  0.0004  0.0005  0.0002 -0.0019]\n",
      " [ 0.007  -0.0036 -0.0001 -0.002  -0.0023]\n",
      " [ 0.003   0.0004  0.0007  0.0001 -0.0028]\n",
      " [-0.0074 -0.0088 -0.0047 -0.0044  0.0145]\n",
      " [ 0.001   0.0009  0.0005  0.0005 -0.0017]\n",
      " [-0.0004  0.0012  0.0004  0.0006 -0.0009]\n",
      " [ 0.0069 -0.0051 -0.0007 -0.0027 -0.0007]\n",
      " [-0.0081 -0.0035 -0.0028 -0.0017  0.0101]\n",
      " [ 0.0188  0.0339  0.0163  0.0171 -0.0482]\n",
      " [-0.0156 -0.0002 -0.0029  0.0002  0.013 ]\n",
      " [-0.0091 -0.0019 -0.0024 -0.0008  0.0093]\n",
      " [ 0.0287 -0.0106  0.0012 -0.006  -0.0132]\n",
      " [ 0.0052  0.0018  0.0017  0.0009 -0.0061]\n",
      " [-0.0144 -0.0139 -0.0079 -0.0069  0.0253]\n",
      " [ 0.0025  0.0022  0.0013  0.0011 -0.0042]\n",
      " [-0.0012  0.0039  0.0013  0.002  -0.0028]\n",
      " [ 0.017  -0.0188 -0.004  -0.01    0.0043]]\n",
      "\n",
      "Sum Difference:\n",
      "7.551716407760978e-08\n",
      "\n",
      "##################################################\n",
      "layer 1 bias_ih gradient:\n",
      "Mine:\n",
      "[-0.0049  0.0143 -0.0051 -0.005  -0.0119 -0.008   0.0313 -0.0039 -0.0008\n",
      " -0.0094  0.0253 -0.0969  0.0387  0.0253 -0.0548 -0.0157  0.0562 -0.0094\n",
      " -0.0029 -0.0138]\n",
      "\n",
      "Pytorch's:\n",
      "[-0.0049  0.0143 -0.0051 -0.005  -0.0119 -0.008   0.0313 -0.0039 -0.0008\n",
      " -0.0094  0.0253 -0.0969  0.0387  0.0253 -0.0548 -0.0157  0.0562 -0.0094\n",
      " -0.0029 -0.0138]\n",
      "\n",
      "Sum Difference:\n",
      "5.3085386753082275e-08\n",
      "\n",
      "##################################################\n",
      "layer 1 bias_hh gradient:\n",
      "Mine:\n",
      "[-0.0049  0.0143 -0.0051 -0.005  -0.0119 -0.008   0.0313 -0.0039 -0.0008\n",
      " -0.0094  0.0253 -0.0969  0.0387  0.0253 -0.0548 -0.0157  0.0562 -0.0094\n",
      " -0.0029 -0.0138]\n",
      "\n",
      "Pytorch's:\n",
      "[-0.0049  0.0143 -0.0051 -0.005  -0.0119 -0.008   0.0313 -0.0039 -0.0008\n",
      " -0.0094  0.0253 -0.0969  0.0387  0.0253 -0.0548 -0.0157  0.0562 -0.0094\n",
      " -0.0029 -0.0138]\n",
      "\n",
      "Sum Difference:\n",
      "5.3085386753082275e-08\n",
      "\n",
      "##################################################\n",
      "layer 0 weight_ih gradient:\n",
      "Mine:\n",
      "[[-0.0007  0.0007  0.0011]\n",
      " [ 0.0003  0.0021 -0.0008]\n",
      " [ 0.0005  0.0009 -0.0011]\n",
      " [-0.0004  0.0003  0.0007]\n",
      " [-0.      0.0003 -0.    ]\n",
      " [-0.0011  0.0011  0.0016]\n",
      " [-0.0005  0.0013  0.0007]\n",
      " [ 0.0003 -0.     -0.0006]\n",
      " [ 0.0002 -0.0004 -0.0003]\n",
      " [ 0.0002 -0.0001 -0.0003]\n",
      " [-0.0004 -0.0004  0.0008]\n",
      " [ 0.0011 -0.0068 -0.0008]\n",
      " [ 0.0012  0.0074 -0.0033]\n",
      " [-0.0062  0.012   0.0085]\n",
      " [-0.0002  0.0012  0.0001]\n",
      " [-0.0078  0.0078  0.012 ]\n",
      " [-0.0027  0.0037  0.0039]\n",
      " [ 0.0021 -0.0014 -0.0033]\n",
      " [ 0.0006 -0.0009 -0.0009]\n",
      " [ 0.0006 -0.0002 -0.0009]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.0007  0.0007  0.0011]\n",
      " [ 0.0003  0.0021 -0.0008]\n",
      " [ 0.0005  0.0009 -0.0011]\n",
      " [-0.0004  0.0003  0.0007]\n",
      " [-0.      0.0003 -0.    ]\n",
      " [-0.0011  0.0011  0.0016]\n",
      " [-0.0005  0.0013  0.0007]\n",
      " [ 0.0003 -0.     -0.0006]\n",
      " [ 0.0002 -0.0004 -0.0003]\n",
      " [ 0.0002 -0.0001 -0.0003]\n",
      " [-0.0004 -0.0004  0.0008]\n",
      " [ 0.0011 -0.0068 -0.0008]\n",
      " [ 0.0012  0.0074 -0.0033]\n",
      " [-0.0062  0.012   0.0085]\n",
      " [-0.0002  0.0012  0.0001]\n",
      " [-0.0078  0.0078  0.012 ]\n",
      " [-0.0027  0.0037  0.0039]\n",
      " [ 0.0021 -0.0014 -0.0033]\n",
      " [ 0.0006 -0.0009 -0.0009]\n",
      " [ 0.0006 -0.0002 -0.0009]]\n",
      "\n",
      "Sum Difference:\n",
      "2.2656422515865415e-08\n",
      "\n",
      "##################################################\n",
      "layer 0 weight_hh gradient:\n",
      "Mine:\n",
      "[[ 0.0004 -0.0001 -0.0002 -0.     -0.0002]\n",
      " [ 0.0002  0.0005 -0.0002  0.0001 -0.0002]\n",
      " [-0.0001  0.0003  0.      0.0001 -0.0001]\n",
      " [ 0.0002 -0.0001 -0.0001 -0.     -0.0001]\n",
      " [ 0.      0.     -0.      0.     -0.    ]\n",
      " [ 0.0006 -0.0001 -0.0003 -0.     -0.0002]\n",
      " [ 0.0004  0.0001 -0.0002  0.     -0.0002]\n",
      " [-0.0002  0.0001  0.0001  0.      0.    ]\n",
      " [-0.0002 -0.      0.0001  0.      0.0001]\n",
      " [-0.0001  0.      0.      0.      0.    ]\n",
      " [ 0.0001 -0.0002 -0.     -0.      0.    ]\n",
      " [-0.0014 -0.0009  0.0009 -0.0002  0.0009]\n",
      " [ 0.0004  0.0016 -0.0005  0.0004 -0.0007]\n",
      " [ 0.0045  0.0005 -0.0025  0.0001 -0.002 ]\n",
      " [ 0.0003  0.0002 -0.0002  0.     -0.0002]\n",
      " [ 0.0047 -0.0007 -0.0025 -0.0002 -0.0017]\n",
      " [ 0.0018 -0.0001 -0.001  -0.     -0.0007]\n",
      " [-0.0012  0.0003  0.0006  0.0001  0.0004]\n",
      " [-0.0004  0.      0.0002  0.      0.0002]\n",
      " [-0.0003  0.0001  0.0001  0.      0.0001]]\n",
      "\n",
      "Pytorch's:\n",
      "[[ 0.0004 -0.0001 -0.0002 -0.     -0.0002]\n",
      " [ 0.0002  0.0005 -0.0002  0.0001 -0.0002]\n",
      " [-0.0001  0.0003  0.      0.0001 -0.0001]\n",
      " [ 0.0002 -0.0001 -0.0001 -0.     -0.0001]\n",
      " [ 0.      0.     -0.      0.     -0.    ]\n",
      " [ 0.0006 -0.0001 -0.0003 -0.     -0.0002]\n",
      " [ 0.0004  0.0001 -0.0002  0.     -0.0002]\n",
      " [-0.0002  0.0001  0.0001  0.      0.    ]\n",
      " [-0.0002 -0.      0.0001  0.      0.0001]\n",
      " [-0.0001  0.      0.      0.      0.    ]\n",
      " [ 0.0001 -0.0002 -0.     -0.      0.    ]\n",
      " [-0.0014 -0.0009  0.0009 -0.0002  0.0009]\n",
      " [ 0.0004  0.0016 -0.0005  0.0004 -0.0007]\n",
      " [ 0.0045  0.0005 -0.0025  0.0001 -0.002 ]\n",
      " [ 0.0003  0.0002 -0.0002  0.     -0.0002]\n",
      " [ 0.0047 -0.0007 -0.0025 -0.0002 -0.0017]\n",
      " [ 0.0018 -0.0001 -0.001  -0.     -0.0007]\n",
      " [-0.0012  0.0003  0.0006  0.0001  0.0004]\n",
      " [-0.0004  0.      0.0002  0.      0.0002]\n",
      " [-0.0003  0.0001  0.0001  0.      0.0001]]\n",
      "\n",
      "Sum Difference:\n",
      "8.144468210957712e-09\n",
      "\n",
      "##################################################\n",
      "layer 0 bias_ih gradient:\n",
      "Mine:\n",
      "[ 0.0008 -0.0023 -0.0019  0.0006 -0.0002  0.0012  0.     -0.0006 -0.0002\n",
      " -0.0003  0.0012  0.0034 -0.0086  0.0024 -0.0006  0.0092  0.0023 -0.003\n",
      " -0.0005 -0.001 ]\n",
      "\n",
      "Pytorch's:\n",
      "[ 0.0008 -0.0023 -0.0019  0.0006 -0.0002  0.0012  0.     -0.0006 -0.0002\n",
      " -0.0003  0.0012  0.0034 -0.0086  0.0024 -0.0006  0.0092  0.0023 -0.003\n",
      " -0.0005 -0.001 ]\n",
      "\n",
      "Sum Difference:\n",
      "8.818460628390312e-09\n",
      "\n",
      "##################################################\n",
      "layer 0 bias_hh gradient:\n",
      "Mine:\n",
      "[ 0.0008 -0.0023 -0.0019  0.0006 -0.0002  0.0012  0.     -0.0006 -0.0002\n",
      " -0.0003  0.0012  0.0034 -0.0086  0.0024 -0.0006  0.0092  0.0023 -0.003\n",
      " -0.0005 -0.001 ]\n",
      "\n",
      "Pytorch's:\n",
      "[ 0.0008 -0.0023 -0.0019  0.0006 -0.0002  0.0012  0.     -0.0006 -0.0002\n",
      " -0.0003  0.0012  0.0034 -0.0086  0.0024 -0.0006  0.0092  0.0023 -0.003\n",
      " -0.0005 -0.001 ]\n",
      "\n",
      "Sum Difference:\n",
      "8.818460628390312e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "input_size = 3\n",
    "hidden_size = 5\n",
    "time_steps = 1\n",
    "num_layers = 2\n",
    "\n",
    "#Pytorch Model\n",
    "torchrnn = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "#My Model\n",
    "myrnn = LSTM(input_size, hidden_size, num_layers)\n",
    "myrnn.torch_weighted(torchrnn)\n",
    "\n",
    "#Inputs\n",
    "torchin = torch.randn(time_steps, batch_size, input_size)\n",
    "htorch = torch.randn(num_layers, batch_size, hidden_size)\n",
    "ctorch = torch.randn(num_layers, batch_size, hidden_size)\n",
    "npin = torchin.detach().numpy()\n",
    "hnp = htorch.detach().numpy()\n",
    "cnp = ctorch.detach().numpy()\n",
    "\n",
    "#targets\n",
    "torchtrue = torch.randint(hidden_size,(batch_size,))\n",
    "nptrue = torchtrue.detach().numpy()\n",
    "\n",
    "#Outputs\n",
    "torchout, (htorch, ctorch) = torchrnn(torchin,(htorch, ctorch))\n",
    "torchout = torchout[-1]\n",
    "torchout.retain_grad()\n",
    "npout, (hnp, cnp) = myrnn(npin, (hnp, cnp))\n",
    "npout = npout[-1]\n",
    "\n",
    "printdif(\"Outputs\",torchout,npout)\n",
    "\n",
    "#Loss\n",
    "torch_loss_fn = nn.CrossEntropyLoss()\n",
    "torchloss = torch_loss_fn(torchout,torchtrue)\n",
    "np_loss_fn = lf.CrossEntropyLoss()\n",
    "nploss = np_loss_fn(npout, nptrue)\n",
    "printdif(\"Loss\", torchloss, nploss)\n",
    "\n",
    "#Calculate gradients\n",
    "torchloss.backward(retain_graph=True)\n",
    "myrnn.backward(np_loss_fn.gradient)\n",
    "\n",
    "#Gradients\n",
    "\n",
    "printdif(\"Output gradient\", torchout.grad, np_loss_fn.gradient)\n",
    "\n",
    "for l in reversed(range(num_layers)):\n",
    "    printdif(f\"layer {l} weight_ih gradient\",\n",
    "             torchrnn.all_weights[l][0].grad,\n",
    "             myrnn.layers[l].gradients[0])\n",
    "    printdif(f\"layer {l} weight_hh gradient\",\n",
    "             torchrnn.all_weights[l][1].grad,\n",
    "             myrnn.layers[l].gradients[1])\n",
    "    printdif(f\"layer {l} bias_ih gradient\",\n",
    "             torchrnn.all_weights[l][2].grad,\n",
    "             myrnn.layers[l].gradients[2])\n",
    "    printdif(f\"layer {l} bias_hh gradient\",\n",
    "             torchrnn.all_weights[l][3].grad,\n",
    "             myrnn.layers[l].gradients[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb9d89",
   "metadata": {},
   "source": [
    "## Comparing outputs and gradients of a 2 layer GRU \n",
    "- doesn't work yet, for some reason grad_z doesn't seem to be accurate even though it's a simple gradient to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510df204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Outputs:\n",
      "Mine:\n",
      "[[ 0.7246  0.4534  0.5055  0.6272 -0.8576]\n",
      " [ 0.7988  0.1712  0.1131  0.4587 -0.0219]]\n",
      "\n",
      "Pytorch's:\n",
      "[[ 0.7246  0.4534  0.5055  0.6272 -0.8576]\n",
      " [ 0.7988  0.1712  0.1131  0.4587 -0.0219]]\n",
      "\n",
      "Sum Difference:\n",
      "2.60770320892334e-07\n",
      "\n",
      "##################################################\n",
      "Loss:\n",
      "Mine:\n",
      "1.279862403869629\n",
      "\n",
      "Pytorch's:\n",
      "1.279862642288208\n",
      "\n",
      "Sum Difference:\n",
      "2.384185791015625e-07\n",
      "\n",
      "h\n",
      "[[ 0.7246  0.4534  0.5055  0.6272 -0.8576]\n",
      " [ 0.7988  0.1712  0.1131  0.4587 -0.0219]]\n",
      "[ 1.5234  0.6246  0.6186  1.086  -0.8795]\n",
      "\n",
      "dh\n",
      "[[ 0.1359  0.1036  0.1092 -0.3767  0.0279]\n",
      " [-0.3432  0.0837  0.079   0.1116  0.069 ]]\n",
      "[-0.2073  0.1873  0.1881 -0.2651  0.0969]\n",
      "\n",
      "r\n",
      "[[0.5449 0.5337 0.6016 0.2643 0.2659]\n",
      " [0.4064 0.4697 0.5425 0.2926 0.4221]]\n",
      "[0.9513 1.0034 1.1441 0.5569 0.688 ]\n",
      "\n",
      "dr\n",
      "[[0.248  0.2489 0.2397 0.1945 0.1952]\n",
      " [0.2412 0.2491 0.2482 0.207  0.2439]]\n",
      "[0.4892 0.498  0.4879 0.4014 0.4391]\n",
      "\n",
      "n\n",
      "[[ 0.7529  0.4885  0.5455  0.0623 -0.1133]\n",
      " [ 0.7819  0.5883  0.0155  0.0838 -0.1151]]\n",
      "[ 1.5348  1.0768  0.561   0.1461 -0.2284]\n",
      "\n",
      "dn\n",
      "[[0.4331 0.7613 0.7024 0.9961 0.9872]\n",
      " [0.3887 0.6539 0.9998 0.993  0.9868]]\n",
      "[0.8218 1.4153 1.7022 1.9891 1.9739]\n",
      "\n",
      "z\n",
      "[[0.2297 0.4195 0.3392 0.6248 0.4875]\n",
      " [0.2808 0.6133 0.545  0.3474 0.5273]]\n",
      "[0.5105 1.0327 0.8842 0.9722 1.0148]\n",
      "\n",
      "dz\n",
      "[[0.1769 0.2435 0.2242 0.2344 0.2498]\n",
      " [0.202  0.2372 0.248  0.2267 0.2493]]\n",
      "[0.3789 0.4807 0.4721 0.4611 0.4991]\n",
      "\n",
      "hprev\n",
      "[[ 0.7246  0.4534  0.5055  0.6272 -0.8576]\n",
      " [ 0.7988  0.1712  0.1131  0.4587 -0.0219]]\n",
      "[ 1.5234  0.6246  0.6186  1.086  -0.8795]\n",
      "\n",
      "grad_z\n",
      "[[-0.0007 -0.0009 -0.001  -0.0499 -0.0052]\n",
      " [-0.0012 -0.0083  0.0019  0.0095  0.0016]]\n",
      "[-0.0019 -0.0092  0.0009 -0.0404 -0.0036]\n",
      "grad_z / B\n",
      "[[-0.0007 -0.0009 -0.001  -0.0499 -0.0052]\n",
      " [-0.0012 -0.0083  0.0019  0.0095  0.0016]]\n",
      "[-0.0009 -0.0046  0.0005 -0.0202 -0.0018]\n",
      "##################################################\n",
      "layer 0 weight_ih gradient:\n",
      "Mine:\n",
      "[[-0.002   0.0011  0.018 ]\n",
      " [ 0.0167  0.0046  0.0011]\n",
      " [ 0.0059  0.002   0.0038]\n",
      " [ 0.0077  0.0024  0.0028]\n",
      " [-0.0096 -0.0022  0.0047]\n",
      " [-0.002  -0.0005  0.0007]\n",
      " [-0.0093 -0.002   0.0061]\n",
      " [ 0.0006 -0.     -0.0019]\n",
      " [-0.0558 -0.0177 -0.0272]\n",
      " [-0.0052 -0.0017 -0.0033]\n",
      " [-0.0349 -0.0009  0.0932]\n",
      " [ 0.0805  0.022   0.0015]\n",
      " [ 0.1013  0.0268 -0.0081]\n",
      " [-0.1127 -0.0411 -0.1125]\n",
      " [ 0.05    0.0118 -0.0197]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.002   0.0011  0.018 ]\n",
      " [ 0.0167  0.0046  0.0011]\n",
      " [ 0.0059  0.002   0.0038]\n",
      " [ 0.0077  0.0024  0.0028]\n",
      " [-0.0096 -0.0022  0.0047]\n",
      " [-0.008  -0.002   0.0021]\n",
      " [-0.016  -0.0034  0.0098]\n",
      " [-0.0003 -0.0005 -0.0039]\n",
      " [-0.0774 -0.026  -0.0531]\n",
      " [-0.0109 -0.0036 -0.0066]\n",
      " [-0.0349 -0.0009  0.0932]\n",
      " [ 0.0805  0.022   0.0015]\n",
      " [ 0.1013  0.0268 -0.0081]\n",
      " [-0.1127 -0.0411 -0.1125]\n",
      " [ 0.05    0.0118 -0.0197]]\n",
      "\n",
      "Sum Difference:\n",
      "0.09063395112752914\n",
      "\n",
      "##################################################\n",
      "layer 0 weight_hh gradient:\n",
      "Mine:\n",
      "[[-0.0055  0.0022  0.0038 -0.0007 -0.0094]\n",
      " [ 0.0102  0.0052  0.0055  0.008  -0.0087]\n",
      " [ 0.0028  0.0024  0.0028  0.0029 -0.005 ]\n",
      " [ 0.0042  0.0028  0.0031  0.0037 -0.0053]\n",
      " [-0.0071 -0.0021 -0.0018 -0.0045  0.0019]\n",
      " [-0.0014 -0.0005 -0.0005 -0.001   0.0006]\n",
      " [-0.0073 -0.0018 -0.0014 -0.0044  0.0009]\n",
      " [ 0.0008 -0.0001 -0.0003  0.0003  0.0008]\n",
      " [-0.0286 -0.021  -0.0241 -0.0269  0.0426]\n",
      " [-0.0025 -0.0021 -0.0024 -0.0025  0.0044]\n",
      " [-0.0132  0.0045  0.0081 -0.0024 -0.0203]\n",
      " [ 0.0257  0.0128  0.0135  0.0199 -0.0212]\n",
      " [ 0.0377  0.0172  0.0176  0.0281 -0.0266]\n",
      " [-0.0101 -0.0133 -0.0164 -0.0136  0.0315]\n",
      " [ 0.0136  0.004   0.0034  0.0086 -0.0035]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.0055  0.0022  0.0038 -0.0007 -0.0094]\n",
      " [ 0.0102  0.0052  0.0055  0.008  -0.0087]\n",
      " [ 0.0028  0.0024  0.0028  0.0029 -0.005 ]\n",
      " [ 0.0042  0.0028  0.0031  0.0037 -0.0053]\n",
      " [-0.0071 -0.0021 -0.0018 -0.0045  0.0019]\n",
      " [-0.0055 -0.0021 -0.002  -0.0038  0.0026]\n",
      " [-0.0123 -0.0033 -0.0026 -0.0075  0.0021]\n",
      " [ 0.0007 -0.0007 -0.0011 -0.0002  0.0024]\n",
      " [-0.0361 -0.0315 -0.0373 -0.0376  0.0679]\n",
      " [-0.0053 -0.0043 -0.005  -0.0053  0.0091]\n",
      " [-0.0132  0.0045  0.0081 -0.0024 -0.0203]\n",
      " [ 0.0257  0.0128  0.0135  0.0199 -0.0212]\n",
      " [ 0.0377  0.0172  0.0176  0.0281 -0.0266]\n",
      " [-0.0101 -0.0133 -0.0164 -0.0136  0.0315]\n",
      " [ 0.0136  0.004   0.0034  0.0086 -0.0035]]\n",
      "\n",
      "Sum Difference:\n",
      "0.10965009033679962\n",
      "\n",
      "##################################################\n",
      "layer 0 bias_ih gradient:\n",
      "Mine:\n",
      "[-0.0058  0.0137  0.0041  0.0058 -0.0091 -0.0019 -0.0092  0.0009 -0.0404\n",
      " -0.0036 -0.0506  0.067   0.0866 -0.0685  0.0463]\n",
      "\n",
      "Pytorch's:\n",
      "[-0.0058  0.0137  0.0041  0.0058 -0.0091 -0.0071 -0.0156  0.0006 -0.0525\n",
      " -0.0076 -0.0506  0.067   0.0866 -0.0685  0.0463]\n",
      "\n",
      "Sum Difference:\n",
      "0.028212392702698708\n",
      "\n",
      "##################################################\n",
      "layer 0 bias_hh gradient:\n",
      "Mine:\n",
      "[-0.0058  0.0137  0.0041  0.0058 -0.0091 -0.0019 -0.0092  0.0009 -0.0404\n",
      " -0.0036 -0.0143  0.0344  0.05   -0.0161  0.0173]\n",
      "\n",
      "Pytorch's:\n",
      "[-0.0058  0.0137  0.0041  0.0058 -0.0091 -0.0071 -0.0156  0.0006 -0.0525\n",
      " -0.0076 -0.0143  0.0344  0.05   -0.0161  0.0173]\n",
      "\n",
      "Sum Difference:\n",
      "0.02821236662566662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "input_size = 3\n",
    "hidden_size = 5\n",
    "time_steps = 1\n",
    "num_layers = 1\n",
    "\n",
    "#Pytorch Model\n",
    "torchrnn = nn.GRU(input_size, hidden_size, num_layers)\n",
    "\n",
    "#My Model\n",
    "myrnn = GRU(input_size, hidden_size, num_layers)\n",
    "myrnn.torch_weighted(torchrnn)\n",
    "\n",
    "#Inputs\n",
    "torchin = torch.randn(time_steps, batch_size, input_size)\n",
    "htorch = torch.randn(num_layers, batch_size, hidden_size)\n",
    "npin = torchin.detach().numpy()\n",
    "hnp = htorch.detach().numpy()\n",
    "\n",
    "#targets\n",
    "torchtrue = torch.randint(hidden_size,(batch_size,))\n",
    "nptrue = torchtrue.detach().numpy()\n",
    "\n",
    "#Outputs\n",
    "torchout, htorch = torchrnn(torchin,htorch)\n",
    "torchout = torchout[-1]\n",
    "torchout.retain_grad()\n",
    "npout, hnp = myrnn(npin, hnp)\n",
    "npout = npout[-1]\n",
    "\n",
    "printdif(\"Outputs\",torchout,npout)\n",
    "\n",
    "#Loss\n",
    "torch_loss_fn = nn.CrossEntropyLoss()\n",
    "torchloss = torch_loss_fn(torchout,torchtrue)\n",
    "np_loss_fn = lf.CrossEntropyLoss()\n",
    "nploss = np_loss_fn(npout, nptrue)\n",
    "printdif(\"Loss\", torchloss, nploss)\n",
    "\n",
    "#Calculate gradients\n",
    "torchloss.backward(retain_graph=True)\n",
    "myrnn.backward(np_loss_fn.gradient)\n",
    "\n",
    "#Gradients\n",
    "\n",
    "# printdif(\"Output gradient\", torchout.grad, np_loss_fn.gradient)\n",
    "\n",
    "for l in reversed(range(num_layers)):\n",
    "    printdif(f\"layer {l} weight_ih gradient\",\n",
    "             torchrnn.all_weights[l][0].grad,\n",
    "             myrnn.layers[l].gradients[0])\n",
    "    printdif(f\"layer {l} weight_hh gradient\",\n",
    "             torchrnn.all_weights[l][1].grad,\n",
    "             myrnn.layers[l].gradients[1])\n",
    "    printdif(f\"layer {l} bias_ih gradient\",\n",
    "             torchrnn.all_weights[l][2].grad,\n",
    "             myrnn.layers[l].gradients[2])\n",
    "    printdif(f\"layer {l} bias_hh gradient\",\n",
    "             torchrnn.all_weights[l][3].grad,\n",
    "         myrnn.layers[l].gradients[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036df25",
   "metadata": {},
   "source": [
    "## Comparing outputs and gradients of a 2 layer RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32174d5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Outputs:\n",
      "Mine:\n",
      "[[-0.6362  0.6754  0.3392  0.8744  0.265 ]\n",
      " [ 0.0648  0.3008  0.6824 -0.7481  0.6173]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.6362  0.6754  0.3392  0.8744  0.265 ]\n",
      " [ 0.0648  0.3008  0.6824 -0.7481  0.6173]]\n",
      "\n",
      "Sum Difference:\n",
      "2.3096799850463867e-07\n",
      "\n",
      "##################################################\n",
      "Loss:\n",
      "Mine:\n",
      "1.4972327947616577\n",
      "\n",
      "Pytorch's:\n",
      "1.4972327947616577\n",
      "\n",
      "Sum Difference:\n",
      "0.0\n",
      "\n",
      "##################################################\n",
      "Output gradient:\n",
      "Mine:\n",
      "[[ 0.0348  0.1293  0.0924 -0.3423  0.0858]\n",
      " [-0.4207  0.1005  0.1471  0.0352  0.1379]]\n",
      "\n",
      "Pytorch's:\n",
      "[[ 0.0348  0.1293  0.0924 -0.3423  0.0858]\n",
      " [-0.4207  0.1005  0.1471  0.0352  0.1379]]\n",
      "\n",
      "Sum Difference:\n",
      "4.470348358154297e-08\n",
      "\n",
      "##################################################\n",
      "layer 1 weight_ih gradient:\n",
      "Mine:\n",
      "[[-0.009  -0.2445  0.0207 -0.3033 -0.2654]\n",
      " [ 0.025   0.0137 -0.0177 -0.002   0.023 ]\n",
      " [ 0.0281  0.0006 -0.019  -0.021   0.0099]\n",
      " [-0.0243  0.0513  0.0133  0.0839  0.047 ]\n",
      " [ 0.0277  0.0054 -0.019  -0.0146  0.0149]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.009  -0.2445  0.0207 -0.3033 -0.2654]\n",
      " [ 0.025   0.0137 -0.0177 -0.002   0.023 ]\n",
      " [ 0.0281  0.0006 -0.019  -0.021   0.0099]\n",
      " [-0.0243  0.0513  0.0133  0.0839  0.047 ]\n",
      " [ 0.0277  0.0054 -0.019  -0.0146  0.0149]]\n",
      "\n",
      "Sum Difference:\n",
      "1.728767529129982e-07\n",
      "\n",
      "##################################################\n",
      "layer 1 weight_hh gradient:\n",
      "Mine:\n",
      "[[-0.0403 -0.112  -0.2788  0.3315 -0.2531]\n",
      " [-0.0388  0.075   0.0862 -0.0069  0.075 ]\n",
      " [-0.0469  0.0789  0.0814  0.0127  0.0702]\n",
      " [ 0.0523 -0.0498 -0.0168 -0.0821 -0.0118]\n",
      " [-0.0452  0.0795  0.0853  0.0059  0.0738]]\n",
      "\n",
      "Pytorch's:\n",
      "[[-0.0403 -0.112  -0.2788  0.3315 -0.2531]\n",
      " [-0.0388  0.075   0.0862 -0.0069  0.075 ]\n",
      " [-0.0469  0.0789  0.0814  0.0127  0.0702]\n",
      " [ 0.0523 -0.0498 -0.0168 -0.0821 -0.0118]\n",
      " [-0.0452  0.0795  0.0853  0.0059  0.0738]]\n",
      "\n",
      "Sum Difference:\n",
      "6.51925802230835e-08\n",
      "\n",
      "##################################################\n",
      "layer 1 bias_ih gradient:\n",
      "Mine:\n",
      "[-0.3982  0.1617  0.1604 -0.0651  0.1651]\n",
      "\n",
      "Pytorch's:\n",
      "[-0.3982  0.1617  0.1604 -0.0651  0.1651]\n",
      "\n",
      "Sum Difference:\n",
      "0.0\n",
      "\n",
      "##################################################\n",
      "layer 1 bias_hh gradient:\n",
      "Mine:\n",
      "[-0.3982  0.1617  0.1604 -0.0651  0.1651]\n",
      "\n",
      "Pytorch's:\n",
      "[-0.3982  0.1617  0.1604 -0.0651  0.1651]\n",
      "\n",
      "Sum Difference:\n",
      "0.0\n",
      "\n",
      "##################################################\n",
      "layer 0 weight_ih gradient:\n",
      "Mine:\n",
      "[[ 0.0756 -0.0657  0.1729]\n",
      " [-0.1152  0.072  -0.003 ]\n",
      " [ 0.0972 -0.0679  0.0688]\n",
      " [ 0.0278 -0.0197  0.0229]\n",
      " [-0.0297 -0.0024  0.1936]]\n",
      "\n",
      "Pytorch's:\n",
      "[[ 0.0756 -0.0657  0.1729]\n",
      " [-0.1152  0.072  -0.003 ]\n",
      " [ 0.0972 -0.0679  0.0688]\n",
      " [ 0.0278 -0.0197  0.0229]\n",
      " [-0.0297 -0.0024  0.1936]]\n",
      "\n",
      "Sum Difference:\n",
      "1.501757651567459e-07\n",
      "\n",
      "##################################################\n",
      "layer 0 weight_hh gradient:\n",
      "Mine:\n",
      "[[ 0.0117  0.1049 -0.0142  0.1241  0.1165]\n",
      " [ 0.0099 -0.0669 -0.0027 -0.0926 -0.0686]\n",
      " [-0.0013  0.0801 -0.0039  0.1027  0.0855]\n",
      " [-0.      0.0241 -0.0014  0.0305  0.0258]\n",
      " [ 0.0232  0.0522 -0.0188  0.0483  0.0637]]\n",
      "\n",
      "Pytorch's:\n",
      "[[ 0.0117  0.1049 -0.0142  0.1241  0.1165]\n",
      " [ 0.0099 -0.0669 -0.0027 -0.0926 -0.0686]\n",
      " [-0.0013  0.0801 -0.0039  0.1027  0.0855]\n",
      " [-0.      0.0241 -0.0014  0.0305  0.0258]\n",
      " [ 0.0232  0.0522 -0.0188  0.0483  0.0637]]\n",
      "\n",
      "Sum Difference:\n",
      "1.0338771971873939e-07\n",
      "\n",
      "##################################################\n",
      "layer 0 bias_ih gradient:\n",
      "Mine:\n",
      "[ 0.2152 -0.039   0.1063  0.034   0.2057]\n",
      "\n",
      "Pytorch's:\n",
      "[ 0.2152 -0.039   0.1063  0.034   0.2057]\n",
      "\n",
      "Sum Difference:\n",
      "4.470348358154297e-08\n",
      "\n",
      "##################################################\n",
      "layer 0 bias_hh gradient:\n",
      "Mine:\n",
      "[ 0.2152 -0.039   0.1063  0.034   0.2057]\n",
      "\n",
      "Pytorch's:\n",
      "[ 0.2152 -0.039   0.1063  0.034   0.2057]\n",
      "\n",
      "Sum Difference:\n",
      "4.470348358154297e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "input_size = 3\n",
    "hidden_size = 5\n",
    "time_steps = 1\n",
    "num_layers = 2\n",
    "\n",
    "#Pytorch Model\n",
    "torchrnn = nn.RNN(input_size, hidden_size, num_layers, )\n",
    "\n",
    "#My Model\n",
    "myrnn = RNN(input_size, hidden_size, num_layers)\n",
    "myrnn.torch_weighted(torchrnn)\n",
    "\n",
    "#Inputs\n",
    "torchin = torch.randn(time_steps, batch_size, input_size)\n",
    "htorch = torch.randn(num_layers, batch_size, hidden_size)\n",
    "npin = torchin.detach().numpy()\n",
    "hnp = htorch.detach().numpy()\n",
    "\n",
    "#targets\n",
    "torchtrue = torch.randint(hidden_size,(batch_size,))\n",
    "nptrue = torchtrue.detach().numpy()\n",
    "\n",
    "#Outputs\n",
    "torchout, htorch = torchrnn(torchin,htorch)\n",
    "torchout = torchout[-1]\n",
    "torchout.retain_grad()\n",
    "npout, hnp = myrnn(npin, hnp)\n",
    "npout = npout[-1]\n",
    "\n",
    "printdif(\"Outputs\",torchout,npout)\n",
    "\n",
    "#Loss\n",
    "torch_loss_fn = nn.CrossEntropyLoss()\n",
    "torchloss = torch_loss_fn(torchout,torchtrue)\n",
    "np_loss_fn = lf.CrossEntropyLoss()\n",
    "nploss = np_loss_fn(npout, nptrue)\n",
    "printdif(\"Loss\", torchloss, nploss)\n",
    "\n",
    "#Calculate gradients\n",
    "torchloss.backward(retain_graph=True)\n",
    "myrnn.backward(np_loss_fn.gradient)\n",
    "\n",
    "#Gradients\n",
    "\n",
    "printdif(\"Output gradient\", torchout.grad, np_loss_fn.gradient)\n",
    "\n",
    "for l in reversed(range(num_layers)):\n",
    "    printdif(f\"layer {l} weight_ih gradient\",\n",
    "             torchrnn.all_weights[l][0].grad,\n",
    "             myrnn.layers[l].gradients[0])\n",
    "    printdif(f\"layer {l} weight_hh gradient\",\n",
    "             torchrnn.all_weights[l][1].grad,\n",
    "             myrnn.layers[l].gradients[1])\n",
    "    printdif(f\"layer {l} bias_ih gradient\",\n",
    "             torchrnn.all_weights[l][2].grad,\n",
    "             myrnn.layers[l].gradients[2])\n",
    "    printdif(f\"layer {l} bias_hh gradient\",\n",
    "             torchrnn.all_weights[l][3].grad,\n",
    "         myrnn.layers[l].gradients[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
